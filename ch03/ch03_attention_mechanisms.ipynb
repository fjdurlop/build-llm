{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a11448-b571-4cd7-a837-dcbdcf6c7a81",
   "metadata": {},
   "source": [
    "# Attention mechanisms\n",
    "\n",
    "1. Simplified self-attention\n",
    "2. Self-attention\n",
    "3. Causal attention\n",
    "4. Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88a9aace-1bb9-435d-b947-69e2dc796a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff525828-40dc-47ab-84a0-90832d3a386e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f6f71c0-1794-4dcf-abb5-7d97d8a7eaf6",
   "metadata": {},
   "source": [
    "## Simplified self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d25d42d-be12-42d7-9bb3-2429b04e0104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# computing attention scores, with dot product:\n",
    "# dot product: similarity function\n",
    "\n",
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05b82c2-c84f-4b96-bd67-15720efa2df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# normalize the attention scores\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cf0270-628c-414e-a518-2593a3a8c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237e1379-2739-456c-9b8b-a449f01f4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8980a795-8387-4ba4-835d-f20fade29752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f90626-b9a8-4046-8ea5-40d125783128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06746c-55da-4cba-8114-bee14340ce78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {
    "cc3fab60-2f81-4a92-8529-ee0d252fae78.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ec0bb156-4ad5-4052-a5dd-8bfd859d24a9",
   "metadata": {},
   "source": [
    "![Screenshot 2025-02-03 at 18.45.35.png](attachment:cc3fab60-2f81-4a92-8529-ee0d252fae78.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8362f0f6-9e59-44ba-afeb-cff37b83dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced17b88-0aec-48a8-810b-aae356fdd078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Computing context vector just for second input\n",
    "\n",
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff00637e-976e-44c6-b3a5-bd58c8b8ccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# unnormalized attention score matrix\n",
    "# attention weights = normalize\n",
    "# context vectors - weighted sum over the inputs\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f657fc0a-de6d-4dc8-8f22-83711d5bf64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# same as above by matrix multiplication \n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5539de-f88e-4588-96fb-422462b2f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# each row sum 1\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035135b4-a07b-4c66-bb3c-bd25db7ef4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "#Quick verification that the values in each row indeed sum to 1:\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b644b40b-934f-47de-a397-bff298c41e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# compute context vector\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6793b9f-79b9-4c8e-b17d-4ac53ed4ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "\n",
    "print(attn_scores.shape)\n",
    "print(attn_weights.shape)\n",
    "print(all_context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea2e8bb-74ba-4d95-9a45-26add61b70c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcfebb-9333-4cb3-ad68-c185798c936b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea21cd8a-5202-4f2f-a8b6-129ff949d572",
   "metadata": {},
   "source": [
    "above there is a simple attention mechanism\n",
    "\n",
    "## Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99b0b1da-2d70-4019-81f5-509136528646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape ftorch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'shape f{inputs.shape}')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a393978-a8e0-419e-8a0a-164a5fed05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70144e-493a-496a-9654-2f116f3bfcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e4a695-d9d4-4644-bfe7-251049614a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16., 33.])\n"
     ]
    }
   ],
   "source": [
    "## note about requires_grad\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "y = x ** 2 + x ** 3 + 1  # y = [5.0, 10.0] # n\n",
    "z = y.sum()     # z = 15.0\n",
    "\n",
    "z.backward()  # Computes dz/dx # Z needs to be scalar\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8e0ba2-ca25-436f-ac7b-a1669f05c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfcfcf2f-5876-4564-add7-9fa790af68d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "#Next we compute the query, key, and value vectors:\n",
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e7b41f9-912f-47b7-81bd-46bfad78d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: weight parameters vs attention weights:\n",
    "# weight parametersare the learned coefficients and attention weights are dynamic, conext-specific values, how a context vector depends on different parts of  the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86be9b05-92b5-49ef-aae7-c1b7c800fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "#As we can see below, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space:\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d8fe22c-4588-476a-864a-2a4b663e7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# In the next step, step 2, we compute the unnormalized attention scores by computing the dot product\n",
    "# between the query and each key vector:\n",
    "\n",
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1febfef-4074-4276-b39f-056d3530b397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#Since we have 6 inputs, we have 6 attention scores for the given query vector:\n",
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given §uery\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94840835-7a9f-4dd4-b3d8-bd6aa44f2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1) # to avoid very small gradients during backpropagation\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c01b5bc-d53f-4220-897a-d8e98667fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "#In step 4, we now compute the context vector for input query vector 2:\n",
    "context_vec_2 = attn_weights_2 @ values # weighting factor that weights the respective imnportance of each value vector\n",
    "print(context_vec_2)\n",
    "\n",
    "# this is just for a context vector, next: to generalize for all context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bca08f4-aa57-41e0-a4c5-f2e0bd993db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: current item the model focuses on\n",
    "# key: used for indexing and searching, key used to match the query\n",
    "# value: similar to key-value pair in dbs, actual content or representation of the inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce583ee-2c3b-4f01-88d7-6a77da611075",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention a python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4716d98-0cce-48cc-b344-c663bf795510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n- get Q,K,V (mutliply by W_*)\\n- Q,K to get attention weight matrix attention between inputs\\n- Multiply V with attention weight matrix to get context vector to each input or all matrix input\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out): # initializes trainable weight matrices\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out)) # instead we can also use nn.Linear\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs)) # second input same as above, above was done step by step\n",
    "\n",
    "# note remember:\n",
    "\"\"\"\n",
    "- get Q,K,V (mutliply by W_*)\n",
    "- Q,K to get attention weight matrix attention between inputs\n",
    "- Multiply V with attention weight matrix to get context vector to each input or all matrix input\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6984bb66-a2df-49a9-8057-7bb5d83ab920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# preferred weight initialization scheme, \n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "# Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because \n",
    "# they use different initial weights for the weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "237239a8-a7b1-4f00-b5f1-63c742391d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2 = SelfAttention_v2(d_in, d_out)\n",
    "sa1 = SelfAttention_v1(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec5ef39d-bb08-4dec-ae5a-5e8dec813b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3074, -0.1773],\n",
       "        [ 0.3485, -0.3441],\n",
       "        [ 0.1583, -0.4312]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa2.W_query.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4de9316-a35e-47ba-a8ca-0cdcf5c8b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3074,  0.3485,  0.1583],\n",
       "        [-0.1773, -0.3441, -0.4312]], requires_grad=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa2.W_query.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0ac29af-27d1-41fe-9084-6a0a8ffd404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.8331, 0.0724],\n",
      "        [0.2255, 0.9237],\n",
      "        [0.3019, 0.3471]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(sa1.W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33c8cbcb-a937-42a4-8108-cd8a532c6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa1.W_query = nn.Parameter(sa2.W_query.weight.T)\n",
    "sa1.W_key   = nn.Parameter(sa2.W_key.weight.T)\n",
    "sa1.W_value = nn.Parameter(sa2.W_value.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93746e9-c602-4f23-835f-4dd1d7bde73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0068cc6e-d0a5-4f8c-b248-f1ba7d8fcb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3074, -0.1773],\n",
      "        [ 0.3485, -0.3441],\n",
      "        [ 0.1583, -0.4312]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(sa1.W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55cf125c-e5bb-4291-aa90-09e905c49eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3671, -0.3086],\n",
      "        [ 0.3675, -0.3095],\n",
      "        [ 0.3675, -0.3094],\n",
      "        [ 0.3670, -0.3073],\n",
      "        [ 0.3670, -0.3061],\n",
      "        [ 0.3672, -0.3085]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sa1(inputs)) # second input same as above, above was done step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a3847-c22d-4c0b-9dc6-1b8161f979f5",
   "metadata": {},
   "source": [
    "## Masked attention scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1043b6da-87a4-4f39-9665-49308858efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse the query and key weight matrices of the\n",
    "# SelfAttention_v2 object from the previous section for convenience\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d0e93b30-54e4-4a53-97ac-be38c44fda71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "# simplest way to mask out future attention weights\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0bc0712-72fb-4a75-8d11-76f9ce153c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f64e3590-22a8-4da9-b397-4546137025d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b22a90c-7ea7-42a0-8cb5-239e77eec4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96d38f08-5909-4c76-8e07-d03f5947eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd43d2fd-211b-441a-8060-cec61bcdba0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Masked attention weights with dropout\n",
    "\n",
    "# example\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
    "example = torch.ones(6, 6) # create a matrix of ones\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208317b3-fc71-4a52-9dfc-51ff0a900646",
   "metadata": {},
   "source": [
    "## Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3e74dabe-8f7e-4ae8-8c7c-551f5ce47250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987390b-c759-493b-bdc1-dc424278c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64f473-ca3d-44ff-807b-918c17ed930d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171300d-fe11-464d-8a67-a547c3a64982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efa9b1-4710-4c46-917d-82d43b84b254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8159438d-57fd-49d9-8c53-6442a019ee0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35b261e8-9fd3-41cf-bb3e-30183e05b68a",
   "metadata": {},
   "source": [
    "## self-attention mechanism with Causal attention and dropout mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f04306d5-70a9-4990-8c4e-614a524fbd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de591145-cb34-4694-850c-b8fe27ae7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de52c7-af8a-413d-9a79-dceca6ea75c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
